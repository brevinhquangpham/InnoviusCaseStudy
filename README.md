# Innovius Technical Case Study

Objective: Develop a solution to compute and visualize similarity scores between companies based on their descriptions.

Technologies used: Pandas, SBERT, plotly, streamlit, nltk (stopwords), sklearn (cosine similarity)

# Feature Extraction
In order to extract features from text data, I utilized SBERT (Sentence BERT) to encode embeddings from the text data. A challenge with SBERT was the fact that SBERT has a token limit of 512, and given that some individual descriptions can have token sizes of up to 1400, this was an issue. To solve this I used a chunking method to encode each 512 token chunk of each description and then mean-pooled the embeddings. SBERT was selected because it has an easily accessible pre-trained model, it is able to understand the context of words in a sentence where Word2Vec and TF-IDF would not, and it is much more efficient than BERT and allows for batch processing. 

# Similarity Evaluation
After I extracted features with SBERT, I used cosine similarity as my similarity metric as it is scale-invariant and it means that it does not worry about the magnitude of the vectors generated by SBERT. The original SBERT paper argues for cosine similarity as the primary metric for the comparison of the similarity of embeddings because it aligns with the properties of the SBERT embedding space where sentence relationships are captured in vector direction rather than magnitude.

# Data Engineering and Scalability
As for a Data Pipeline that would be used for company similarity processing, it would consist of these components/microservices:
[Data Sources] -> [Data Ingestion Layer] -> [Processing Layer] -> [Storage Layer] -> [API Layer] -> [Client Applications]

- Ingestion:
For ingestion, I would use something like Apache Kafka to allow for an event-driven ingestion architecture with company data. I would have to ensure several things. Firstly, I would have to ensure that each company is treated as an independent event, with unique ids. Secondly, I would use a form of schema validation to ensure whatever is sent to our Kafka topics adheres to the data ingestion schema, which would be a json format with the columns of the company description dataframe as keys. Finally, I would use a DLQ to ensure that events that fail ingestion or processing are saved and scanned for issues.

- Processing:
For processing I would use a similar method, but ensure that I can utilize distributed computing and batch processing with something like Apache Spark to optimize speed. Additionally, I could preprocess the top 5 most similar companies for each company.

- Storage
For storage I would use a SQL database and implement sharding.

As for other scalability issues, it would be best to use Redis for caching on popular similarity scores and implement some form of cache invalidation. I would also precompute similarity scores for companies that are frequently compared with one another.

# Visualization
In order to make this quick and easy I used Streamlit for visualization. Streamlit is very quick and easy, but that comes with some negative side effects such as me not being as comfortable in it as I am something like React or Svelte. Other issues with the visualization include the fact that it runs pretty slowly all things considered and would not under any circumstance be able to survive any amounts of high traffic.

## Next Steps
For next steps and improvement, I would implement more rigorous testing and quality assurance. In this example to test this all I did was do an eyeball test to ensure that the top 5 similar companies that were received were similar, but I would use something akin to a TREC style evaluation metric if I had more time. Additionally, I would work to fine tune the SBERT model such that it is able to more accurately fit our needs.

Note on including my data in the Github:
I understand that including my data in my Github is typically bad practice, but as this is a case study exercise I did so to reduce overhead.
